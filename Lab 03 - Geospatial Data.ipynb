{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e448fcf-b508-468b-8afd-281bca379a6d",
   "metadata": {},
   "source": [
    "# Lab 03 - Working with and Visualizing Geospatial Data\n",
    "\n",
    "In this notebook, we will learn about `geopandas` - a Python library specialized in reading and processing geospatial vector data files such as **Shapefiles** and **GeoJSON** files.\n",
    "\n",
    "First, ensure that you have `geopandas` installed in your environment. Note that `geopandas` can be quite fussy with environments and operating systems.\n",
    "\n",
    "The suggested way for Windows-based installations is to ensure that all your libraries are using the same channel (i.e. `conda-forge`)\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge geopandas\n",
    "```\n",
    "\n",
    "For those using `pip`, you may also install simply using \n",
    "```bash\n",
    "pip install geopandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50b6cb-adfb-4b95-b0ac-eb9991724ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this cell if running on Google Colab ONLY!\n",
    "# !pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f846cc-c083-422a-90e9-e3ba8a789ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63eca4e-f5b5-453c-a442-e596fc331736",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only do this if you're working on Google Colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')\n",
    "# %cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca8a251-81b6-4d33-89eb-bb44b32b42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when using Google Colab\n",
    "# dataset_folder = Path('/gdrive/MyDrive/datasets')\n",
    "\n",
    "# when using local folder\n",
    "dataset_folder = Path('datasets/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca9773-b98d-4471-9910-09e869073cc2",
   "metadata": {},
   "source": [
    "## Shapefiles\n",
    "\n",
    "As defined by ArcGIS:\n",
    "> A shapefile is an Esri vector data storage format for storing the location, shape, and attributes of geographic features. It is stored as a set of related files and contains one feature class.\n",
    ">\n",
    "> The primary way to make shapefile data available for others to view through a web browser is to add it to a `.zip` file, upload it, and publish a hosted feature layer. The `.zip` file must contain at least the `.shp`, `.shx`, `.dbf`, and `.prj` files components of the shapefile.\n",
    "\n",
    "For more details, see the [main documentation](https://doc.arcgis.com/en/arcgis-online/reference/shapefiles.htm).\n",
    "\n",
    "### GADM Dataset\n",
    "\n",
    "Go to [GADM](https://gadm.org/download_country.html) and download the [Philippines shapefiles](https://geodata.ucdavis.edu/gadm/gadm4.1/shp/gadm41_PHL_shp.zip).\n",
    "\n",
    "Unzip the file into a folder and make sure all files are within the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdeb7b-a6ea-4331-b6bd-bbf6cdfaf34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_gdf = gpd.read_file(dataset_folder / 'gadm41_PHL_shp/gadm41_PHL_1.shp')\n",
    "ph_gdf.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66942d3e-6025-47a7-aafd-c645c4d63dc4",
   "metadata": {},
   "source": [
    "### Education Facilities Point Data\n",
    "\n",
    "The provided files were downloaded from the Humanitarian Data Exchange website as extracted by the Humanitarian OpenStreetMap Team (HOTOSM):\n",
    "* [PHL South Education Facilities](https://data.humdata.org/dataset/hotosm_phl_south_education_facilities)\n",
    "* [PHL South Education Facilities](https://data.humdata.org/dataset/hotosm_phl_north_education_facilities)\n",
    "\n",
    "To simplify the dataset for use in the Plotly Dash app, we'll concatenate these two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a875756-4622-4c89-978f-b0af8eafb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_s = gpd.read_file(dataset_folder / 'hotosm_phl_south_education_facilities_points_shp/hotosm_phl_south_education_facilities_points.shp')\n",
    "educ_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f61bc-aeab-41d8-acdc-9ea611572445",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6244966-f01b-474e-a48e-f30d52962812",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_n = gpd.read_file(dataset_folder / 'hotosm_phl_north_education_facilities_points_shp/hotosm_phl_north_education_facilities_points.shp')\n",
    "educ_n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e476d-8af7-442b-b3c3-68bd11e3367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce419f7-745b-4f44-bdeb-853734b43163",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_sites = pd.concat([educ_n, educ_s])\n",
    "educ_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc3fbb-6b9b-4b1f-a263-2139e2776882",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_sites.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafc534-8612-437e-8abd-6b614db95f34",
   "metadata": {},
   "source": [
    "## Visualizing geospatial data using `geopandas`\n",
    "\n",
    "`geopandas` provides a simple and straightforward way to visualize the geographic boundaries and any underlying column information. \n",
    "\n",
    "However, note that there are some data transformations that would be needed for creating choropleth maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba059c96-95fc-40a8-97b2-5c23cd93f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ph_gdf.plot(figsize=(20, 14), color='white', edgecolor='dimgray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eca6ae-b63c-4d87-804c-924024cd3aea",
   "metadata": {},
   "source": [
    "### Figure 1. Base map of the administrative boundaries of the Philippines (provincial level)\n",
    "\n",
    "This visualization only shows the geometries of the boundaries of each province in the Shapefile. The dataset does not have any quantitative or categorical attribute that can be visualized. This shapefile is usually used as a base for creating choropleth maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998156cd-de48-4cf3-bf90-fd7477b32874",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ph_gdf.plot(figsize=(20, 14), color='white', edgecolor='dimgray')\n",
    "educ_sites.plot(ax=ax, column='amenity', legend=True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a4887-b249-45e5-aaa1-f5da4fd5ca33",
   "metadata": {},
   "source": [
    "### Figure 2. Scatter Map of the Education Facilities in the Philippines\n",
    "\n",
    "This is a two-layer visualization of the education facilities dataset on top of the administrative boundaries dataset.\n",
    "\n",
    "In `geopandas`, visualizing the administrative boundaries provides the context for the scatter points. Without it, the graph would look like the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46356eef-8b55-434d-ac2f-c49b935c8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_sites.plot(figsize=(20,14), column='amenity', legend=True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e92a0-40b2-48a0-a1c6-1d4e4f2eeba7",
   "metadata": {},
   "source": [
    "### Save the merged dataset for a single loading in the Plotly Dash application\n",
    "\n",
    "`geopandas` provides a way to save any changes you've made to the `GeoDataFrame`. You may save it as a Shapefile or a GeoJSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd5d6c-3929-4216-8644-192f395e838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapefile\n",
    "educ_sites.to_file(dataset_folder / 'hotosm_phl_education_facilities.shp')\n",
    "\n",
    "# geojson\n",
    "educ_sites.to_file(dataset_folder / 'hotosm_phl_education_facilities.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd471a-ca43-48b9-9bc9-39cfa6a024b8",
   "metadata": {},
   "source": [
    "## How to make a choropleth map?\n",
    "\n",
    "Given the dataset that we have, it is not ready for a choropleth map. But we can **derive data** from the current point data we have and the polygon administrative boundaries.\n",
    "\n",
    "A common structure for the dataset to easily make a choropleth map would be to have:\n",
    "* each location should be **ONE** row.\n",
    "* each quantitative attribute about that location is a **COLUMN**.\n",
    "\n",
    "This way, it's easy to access the location as a *key* from the row and also visualize and entire column variable for a map. It also prepares your data for easy interactions later by selecting only the column needed for the visual.\n",
    "\n",
    "### How to transform the data?\n",
    "**First**, identify the administrative boundary level for aggregation. Make sure that you have `Polygon` type geometry for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177302e9-4f10-4d2d-9fbd-5e155d5547e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_gdf.plot(figsize=(20, 14), color='white', edgecolor='dimgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162eef8-b6ed-4fc5-8ab5-85a9f4479060",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_gdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73501128-c6d5-48b4-9b0e-0df73d0d7da3",
   "metadata": {},
   "source": [
    "In our case, since we already have the provincial level administrative boundaries, we'll use this for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc78c7fd-aa43-4fd7-9982-79872bbf6e7a",
   "metadata": {},
   "source": [
    "**Next**, determine how you can aggregate your observation data (usually `Point`) or with longitude and latitude values into the regions you've identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073e333-8ec1-4be5-830b-cb972d7437a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_sites.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf45a1-363a-46f8-aff4-660d52088fa9",
   "metadata": {},
   "source": [
    "Looking at our education facilities dataset, we don't have a column that matches the `NAME_1` attribute of `ph_gdf`. We don't have a text field that can be used for a `pandas` merge.\n",
    "\n",
    "Luckily, since we're working with two `GeoDataFrame`s with geometries, we can perform spatial join.\n",
    "\n",
    "We can check whether the `Polygon` **contains** the `Points` or if they simply **intersect**.\n",
    "\n",
    "By performing a spatial join, we're assigning a province value to the educational facility in the dataset.\n",
    "\n",
    "This is achieved using the `gdp.sjoin()` method. [Documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1759656-43a6-4f2f-80df-b39954caecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_intersect = gpd.sjoin(ph_gdf, # left geodataframe (the polygons will be kept)\n",
    "                               educ_sites, # right geodataframe (the ponit geometry will be dropped, but all attributes will be kept)\n",
    "                               how='left', # default is 'inner' but we want to keep all the polygons! we don't want a map with a hole!\n",
    "                               predicate='intersects' # default\n",
    "                              )\n",
    "ph_educ_site_intersect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473ea24-36a9-43e8-858a-2e44474e6f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_intersect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a9359f-f318-49b1-a4bd-fe858c745d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_sites.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337439ee-13be-47a2-a9b1-81bb2def951d",
   "metadata": {},
   "source": [
    "### WAIT! Why do we have more eductional facilities?!\n",
    "\n",
    "Since we used the predicate or spatial operation **intersects**, if the point falls on the border or a corner of two or more polygons, then that point will be assigned to all the polygons it intersects with.\n",
    "\n",
    "Let's try contains to see if we're able to keep the same number of facilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2c750-a884-4196-93d2-86bbbed84078",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains = gpd.sjoin(ph_gdf, # left geodataframe (the polygons will be kept)\n",
    "                               educ_sites, # right geodataframe (the ponit geometry will be dropped, but all attributes will be kept)\n",
    "                               how='left', # default is 'inner' but we want to keep all the polygons! we don't want a map with a hole!\n",
    "                               predicate='contains' # points should be within the polygon\n",
    "                              )\n",
    "ph_educ_site_contains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44345c6-998b-417d-9015-22784d92b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77300a8-6cdc-4c3a-ae9d-d9d295e175cc",
   "metadata": {},
   "source": [
    "### Duplicated data problem\n",
    "\n",
    "It seems like whether we use **contains** or **intersects**, the output is the same. How can we fix this?\n",
    "\n",
    "We can opt to drop_duplicates based on the `osm_id` subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730c6e3-ef01-40ab-8afd-244facb1467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains[ph_educ_site_contains.duplicated(subset='osm_id', keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e223d85-3a9d-44fe-bcff-e2dfd3bd1d79",
   "metadata": {},
   "source": [
    "Looking at the patterns, we can see that the school names are repeating, but the `NAME_1` is also identical. From this, we can potentially deduce that there may already be duplicates from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96325dd-2d2c-4c3f-a762-a9454c3c8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_sites.duplicated(subset='osm_id').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1ea31-8088-4e2a-bc15-2e28f771ad9b",
   "metadata": {},
   "source": [
    "Let's drop the duplicates and re-run the `sjoin` (for contains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb2a5c-76ea-4c70-9a82-200a85f7cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_sites = educ_sites.drop_duplicates()\n",
    "educ_sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb25660-762b-4e38-b2c4-1d8e285d03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains = gpd.sjoin(ph_gdf, # left geodataframe (the polygons will be kept)\n",
    "                               educ_sites, # right geodataframe (the ponit geometry will be dropped, but all attributes will be kept)\n",
    "                               how='left', # default is 'inner' but we want to keep all the polygons! we don't want a map with a hole!\n",
    "                               predicate='contains' # points should be within the polygon\n",
    "                              )\n",
    "ph_educ_site_contains.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1cc2b-c9e8-4fd7-9b21-65f10a4c94f8",
   "metadata": {},
   "source": [
    "The dataset seems to be now missing **9 rows**. This may warrant further investigation for data cleaning and processing. But for the purpose of this lesson, we'll focus on creating the dataset for the choropleth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b001bf38-c1ec-4ccd-b823-338085a92a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815cbd68-8a3c-47db-91a6-2e911ee12854",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains.geometry.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cef691-cc61-4a3c-b730-cc12a2f98556",
   "metadata": {},
   "source": [
    "As mentioned in the comments, the `geometry` of the left `GeoDataFrame` will be the one left. The Point information is now dropped.\n",
    "\n",
    "This is because a `GeoDataFrame` can only have **ONE** geometry information.\n",
    "\n",
    "But looking at the dataset, we can see that the location information is now repeating(!) but what we want is **ONE** row per location (`NAME_1`).\n",
    "\n",
    "We can now work with `groupby` to get the data that we need.\n",
    "\n",
    "But first, let's check if we can create more columns than just `value_counts` per location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7f796-7b53-462b-b4a3-db26d980734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f4f01-ffdc-4c82-87c5-2954c9e17592",
   "metadata": {},
   "source": [
    "### Extracting categorical variables for counts\n",
    "\n",
    "When working with geospatial data, usually locations have categories that can be used for filtering! So in our case, we can try to extract the information about the following:\n",
    "* amenity\n",
    "* building\n",
    "* operatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbfce7-6acd-4bce-8dd5-b3b7e671168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains.amenity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab841f-00fc-477f-84db-a00f0f016bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains.building.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d387a-1860-479e-845c-00eeb5ba98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains.operatorty.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd7cda-748d-4769-9bd7-7ec7f8dd3396",
   "metadata": {},
   "source": [
    "Looking at all of these, it seems like some further data cleaning may need to be done for the `amenity` column.\n",
    "\n",
    "The `building` column is a bit tricky to use since there are values like *house* and *yes*.\n",
    "\n",
    "For the `operatory`, most of the values are quite appropriate, so we can also create categories from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cfc15-ad05-4c63-9f05-788cdee591f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_educ_site_contains[ph_educ_site_contains.amenity == 'bus_station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69940dc9-ee31-4468-ace8-47b84d0e8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ph_educ_site = ph_educ_site_contains[ph_educ_site_contains.amenity != 'bus_station']\n",
    "clean_ph_educ_site.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7df711-68a2-4ab8-af1c-8ff885908cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenity_group = clean_ph_educ_site.groupby(['NAME_1','amenity'])['osm_id'].count().reset_index()\n",
    "amenity_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247b4430-4256-42f4-b876-3b08877faf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatorty_group = clean_ph_educ_site.groupby(['NAME_1','operatorty'])['osm_id'].count().reset_index()\n",
    "operatorty_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0fdd2-423e-4458-8f86-473765be3851",
   "metadata": {},
   "source": [
    "Now that we have the counts per category, we can now create pivot tables to be used for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f0ec0-7c68-4988-ae61-bc69ce377f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenity_pivot = pd.pivot_table(amenity_group, index='NAME_1', columns='amenity', values='osm_id', fill_value=0)\n",
    "amenity_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079fc4d8-f731-49d4-8b7d-0e934f4d8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatorty_pivot = pd.pivot_table(operatorty_group, index='NAME_1', columns='operatorty', values='osm_id', fill_value=0)\n",
    "operatorty_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6231bd-f43b-45fc-b5be-ce019c805a32",
   "metadata": {},
   "source": [
    "Since there's two `university` values for both, let's keep them as separate `DataFrames` instead of merging them.\n",
    "\n",
    "What we need to do next is to **merge this with the `ph_gdf` `GeoDataFrame`** so that we have a geospatial dataset to work with instead of just a regular `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7ddaf-cbf0-4194-83e7-9dff45e953e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: The GeoDataFrame should be on the OUTSIDE (left) of the merge so you keep the GeoDataFrame type\n",
    "amenity_gdf = ph_gdf.merge(amenity_pivot, left_on='NAME_1', right_index=True, how='left')\n",
    "amenity_gdf.fillna(0, inplace=True)\n",
    "amenity_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362aa76d-90e7-41a6-b61e-c23f8a9cfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: The GeoDataFrame should be on the OUTSIDE (left) of the merge so you keep the GeoDataFrame type\n",
    "operatorty_gdf = ph_gdf.merge(operatorty_pivot, left_on='NAME_1', right_index=True, how='left')\n",
    "operatorty_gdf.fillna(0, inplace=True)\n",
    "operatorty_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b78a35-31ac-416e-9b99-c959497174c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ph_gdf.shape)\n",
    "print(amenity_gdf.shape)\n",
    "print(operatorty_gdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac2d22-c909-4361-a1f7-db1a99d69ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenity_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca302a5b-9b55-448f-9cd2-7c4a09582e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenity_gdf = amenity_gdf[['GID_1', 'GID_0', 'COUNTRY', 'NAME_1', 'geometry', 'college',\n",
    "       'kindergarten', 'school', 'university']]\n",
    "amenity_gdf.columns = ['gid_1', 'gid_0', 'country', 'province', 'geometry', 'college',\n",
    "       'kindergarten', 'school', 'university']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd952a-05bb-4991-a74d-3f84952da905",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatorty_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8b3c4-8ecb-4940-bc6b-bb8edab1ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatorty_gdf = operatorty_gdf[['GID_1', 'GID_0', 'COUNTRY', 'NAME_1', 'geometry',\n",
    "       'consortium', 'corporation', 'government', 'private', 'public',\n",
    "       'religious', 'university']]\n",
    "operatorty_gdf.columns = ['gid_1', 'gid_0', 'country', 'province', 'geometry',\n",
    "       'consortium', 'corporation', 'government', 'private', 'public',\n",
    "       'religious', 'university']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c602ce-894b-4bb7-936c-304cf4499090",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenity_gdf.plot(figsize=(20, 14), column='kindergarten', legend=True)\n",
    "plt.title('Kindergarten');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db67024-e4f9-4382-b138-d9297bfe00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatorty_gdf.plot(figsize=(20, 14), column='religious', legend=True)\n",
    "plt.title('Operatorty: Religious');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66ccb6-0676-400a-9229-b57474dd55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatorty_gdf.plot(figsize=(20, 14), column='public', legend=True)\n",
    "plt.title('Operatorty: Public');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcab1b3-3d35-4284-bb5e-ebc7cf6e9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatorty_gdf.plot(figsize=(20, 14), column='private', legend=True)\n",
    "plt.title('Operatorty: Private');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08df62-8a01-408c-8d27-4e03204e92bd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "When making choropleth maps, it's not usually the case that you would already have the data in the right structure.\n",
    "\n",
    "It might be possible when working with time series data. In that case, you would need to have:\n",
    "* one location for each row\n",
    "* one year/month/day for each column (also achieved by creating a pivot table > merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5e19f-919a-4348-803c-22ca2806e86b",
   "metadata": {},
   "source": [
    "## Save the transformed data\n",
    "\n",
    "One of the reasons why we chose to drop the unnecessary columns is to make the dataset file as small as possible so that when it's used for the web visualization, it would not cause a big slowdown to the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2bac8e-8571-4c47-b531-2abab13f1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenity_gdf.to_file(dataset_folder / 'ph_educ_by_amenity.geojson', driver='GeoJSON')\n",
    "\n",
    "operatorty_gdf.to_file(dataset_folder / 'ph_educ_by_operatorty.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa0ddb-33b1-4fb7-a752-91c8d7246968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
